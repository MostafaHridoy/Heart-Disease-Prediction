# -*- coding: utf-8 -*-
"""CSE475.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sTLOUUGIxJxLYZ3J0xzy1WyZXY59UPh6
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Reading the dataset"""

df=pd.read_csv("//content//drive//MyDrive//heart_Testing.csv")
df

df.columns

df.shape

df.describe()

df.info(memory_usage='deep')

for column in df:
  if df[column].dtype == 'float64':
      df[column]=pd.to_numeric(df[column], downcast='float')
  if df[column].dtype == 'float32':
    df[column]=pd.to_numeric(df[column], downcast='float')
  if df[column].dtype == 'int64':
      df[column]=pd.to_numeric(df[column], downcast='integer')

df.info(memory_usage='deep')

df.max()

df.corr()

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(), annot=True, cmap="PiYG", linewidths=0.9)

df['target'].unique()

df['target'].value_counts().plot.bar()

for label in df.columns:
    print(label,':',len(df[label].unique()))

df.isnull().sum()

df.duplicated().sum()

df=df.drop_duplicates()

df

df.duplicated().sum()

df.shape

df.skew()

sns.distplot(df.skew(),hist=False)
plt.show()

"""Checking for outliers"""

colNames=df.columns.to_list()

plt.figure(figsize=(10,10))
df.boxplot()

df.hist(figsize=(15,14), layout=(4,4))
plt.show()

columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']

for feature in columns:       
    sns.distplot(df[feature])
    plt.show()

def ReplaceOutliers(df):
  for col in columns:

      q1=df[col].quantile(0.25)
      q3=df[col].quantile(0.75)
      IQR=q3-q1
      upperBound=q3+(1.5*IQR)
      lowerBound=q1-(1.5*IQR)
      df[col]=np.where(df[col]>upperBound,upperBound,
         np.where(df[col]<lowerBound,lowerBound,df[col]))
  return df

df=ReplaceOutliers(df)

plt.figure(figsize=(10,10))
df.boxplot()

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()
df['target']=label_encoder.fit_transform(df['target'])

df

from sklearn.preprocessing import StandardScaler
StandardScaler = StandardScaler()  
columns_to_scale = columns
df[columns_to_scale] = StandardScaler.fit_transform(df[columns_to_scale])
df[columns_to_scale]

X = df.drop('target',axis=1)
Y = df['target']

from sklearn.model_selection import train_test_split

X_train, X_test,y_train, y_test=train_test_split(X,Y,test_size=0.3,random_state=42)

print('X_train-', X_train.size)
print('X_test-',X_test.size)
print('y_train-', y_train.size)
print('y_test-', y_test.size)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import r2_score #r2_score is the correlation co-efficient.It tells how the data is connected.
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve,auc

"""CLASSIFICATION MODELS"""

accuracy=[]
R2_Score=[]
F1_Score=[]
PRECISION=[]
RECALL=[]

"""RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(max_depth=9,n_estimators=100)
mrfc=rfc.fit(X_train,y_train)
p=mrfc.predict(X_test)
cmat=confusion_matrix(y_test,p)
cmat

sns.heatmap(cmat,annot=True,cmap="rocket")

from sklearn.metrics import classification_report
print(classification_report(y_test,p))

print("Accuracy Score : ",accuracy_score(y_test,p))

accuracyRandomForest=accuracy_score(y_test,p)
accuracy.append(accuracyRandomForest)

print("R2_SCORE :",r2_score(y_test,p))

r2ScoreRandomForest=r2_score(y_test,p)
R2_Score.append(r2ScoreRandomForest)

print("Precision: ", metrics.precision_score(y_test, p))

precisionRandomForest=metrics.precision_score(y_test, p)
PRECISION.append(precisionRandomForest)

print("Recall: ", metrics.recall_score(y_test, p))

recallRandomForest=metrics.precision_score(y_test, p)
RECALL.append(recallRandomForest)

print("F1_SCORE :",f1_score(y_test,p))

f1ScoreRandomForest=metrics.precision_score(y_test, p)
F1_Score.append(f1ScoreRandomForest)

"""ROC_AUC CURVE For Random Forest Classifier"""

prob_rf=mrfc.predict_proba(X_test)
auc_lr = metrics.roc_auc_score(y_test,prob_rf[:,1])
fprlr,tprlr,_ = roc_curve(y_test,prob_rf[:,1])
roc_auc=auc(fprlr,tprlr)
plt.plot(fprlr,tprlr,label = "AUC = %.2f" % auc_lr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Random Forest")
plt.plot([0,1],[0,1],label='baseline',
         linestyle='--')
plt.legend()
plt.show()

"""DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
m=dt.fit(X_train,y_train)
predictor=m.predict(X_test)
cm=confusion_matrix(y_test,predictor)
cm

sns.heatmap(cm,annot=True,cmap="terrain")

from sklearn.tree import DecisionTreeClassifier, plot_tree

print(classification_report(y_test,predictor))
plt.figure(figsize=(10,10))
clf = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)
plot_tree(clf, filled=True, fontsize=10)
plt.show()

print("Accuracy Score : ",accuracy_score(y_test,predictor))

accuracyDecisionTree=accuracy_score(y_test,predictor)
accuracy.append(accuracyDecisionTree)

print("R2_SCORE :",r2_score(y_test,predictor))

r2ScoreDecisionTree=r2_score(y_test,predictor)
R2_Score.append(r2ScoreDecisionTree)

print("Precision: ", metrics.precision_score(y_test, predictor))

precisionDecisionTree=metrics.precision_score(y_test, predictor)
PRECISION.append(precisionDecisionTree)

print("Recall: ", metrics.recall_score(y_test, predictor))

recallDecisionTree=metrics.recall_score(y_test, predictor)
RECALL.append(recallDecisionTree)

print("F1_SCORE :",f1_score(y_test,predictor))

f1scoreDecisionTree=f1_score(y_test,predictor)
F1_Score.append(f1scoreDecisionTree)

"""ROC_AUC Curve for Decision Tree"""

prob_dt= clf.predict_proba(X_test)
auc_dt = metrics.roc_auc_score(y_test,prob_dt[:,1])
fprdt,tprdt,_= roc_curve(y_test,prob_dt[:,1])
roc_auc_dt=auc(fprdt,tprdt)
plt.plot(fprdt,tprdt,label = "AUC = %.2f" % auc_dt)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Decision Tree")
plt.plot([0,1],[0,1],label='baseline',
         linestyle='--')
plt.legend()
plt.show()

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
KNN=KNeighborsClassifier()
model=KNN.fit(X_train,y_train)
pred= model.predict(X_test)
c=confusion_matrix(y_test,pred)
c

sns.heatmap(c,annot=True,cmap="rocket")

print(classification_report(y_test,pred))

print("Accuracy Score : ",accuracy_score(y_test,pred))

accuracyKNN=accuracy_score(y_test,pred)
accuracy.append(accuracyKNN)

print("R2_SCORE :",r2_score(y_test,predictor))

r2ScoreKNN=r2_score(y_test,predictor)
R2_Score.append(r2ScoreKNN)

print("Precision: ", metrics.precision_score(y_test, pred))

precisionKNN=metrics.precision_score(y_test, pred)
PRECISION.append(precisionKNN)

print("Recall: ", metrics.recall_score(y_test, pred))

recallKNN=metrics.recall_score(y_test, pred)
RECALL.append(recallKNN)

print("F1_SCORE :",f1_score(y_test,pred))

f1scoreKNN=f1_score(y_test,pred)
F1_Score.append(f1scoreKNN)

"""ROC_AUC Curve for KNN"""

prob_knn= model.predict_proba(X_test)
auc_knn = metrics.roc_auc_score(y_test,prob_knn[:,1])
fprknn,tprknn,_= roc_curve(y_test,prob_knn[:,1])
roc_auc_knn=auc(fprknn,tprknn)
plt.plot(fprknn,tprknn,label = "AUC = %.2f" % auc_knn)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for KNN")
plt.plot([0,1],[0,1],label='baseline',
         linestyle='--')
plt.legend()
plt.show()

"""NAIVE BAYES"""

from sklearn.naive_bayes import GaussianNB
NB=GaussianNB()
modelNV=NB.fit(X_train,y_train)
prediction=modelNV.predict(X_test)
cNV=confusion_matrix(y_test,prediction)
cNV

sns.heatmap(cNV,annot=True,cmap="terrain")

print(classification_report(y_test,prediction))

print("Accuracy Score : ",accuracy_score(y_test,prediction))

accuracyNV=accuracy_score(y_test,prediction)
accuracy.append(accuracyNV)

print("R2_SCORE :",r2_score(y_test,prediction))

r2ScoreNV=r2_score(y_test,prediction)
R2_Score.append(r2ScoreNV)

print("Precision: ", metrics.precision_score(y_test, prediction))

precisionNV=metrics.precision_score(y_test, prediction)
PRECISION.append(precisionNV)

print("Recall: ", metrics.recall_score(y_test, prediction))

recallNV=metrics.recall_score(y_test, prediction)
RECALL.append(recallNV)

print("F1_SCORE :",f1_score(y_test,prediction))

f1ScoreNV=f1_score(y_test,prediction)
F1_Score.append(f1ScoreNV)

"""ROC_AUC Curve for Naive Bayes"""

prob_gnb= NB.predict_proba(X_test)
auc_gnb = metrics.roc_auc_score(y_test,prob_gnb[:,1])
fprgnb,tprgnb,_= roc_curve(y_test,prob_gnb[:,1])
roc_auc_gnb=auc(fprgnb,tprgnb)
plt.plot(fprgnb,tprgnb,label = "AUC = %.2f" % auc_gnb)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Naive-Bayes")
plt.plot([0,1],[0,1],label='baseline',
         linestyle='--')
plt.legend()
plt.show()

"""SUPPORT VECTOR MACHINE"""

from sklearn.svm import SVC
svm=SVC(C=1,kernel='linear',gamma='auto',probability=True)
modelSVC=svm.fit(X_train,y_train)
predictionSVC=modelSVC.predict(X_test)
cmSVC= confusion_matrix(y_test,predictionSVC)
cmSVC

sns.heatmap(cmSVC,annot=True,cmap="rocket")

print(classification_report(y_test,predictionSVC))

print("Accuracy Score : ",accuracy_score(y_test,predictionSVC))

accuracySM=accuracy_score(y_test,predictionSVC)
accuracy.append(accuracySM)

print("R2_SCORE :",r2_score(y_test,predictionSVC))

r2ScoreSM=r2_score(y_test,predictionSVC)
R2_Score.append(r2ScoreSM)

print("Precision: ", metrics.precision_score(y_test, predictionSVC))

precisionSM=metrics.precision_score(y_test, predictionSVC)
PRECISION.append(precisionSM)

print("Recall: ", metrics.recall_score(y_test, predictionSVC))

recallSM=metrics.recall_score(y_test, predictionSVC)
RECALL.append(recallSM)

print("F1_SCORE :",f1_score(y_test,predictionSVC))

f1sm=f1_score(y_test,predictionSVC)
F1_Score.append(f1sm)

"""ROC_AUC Curve for Support Vector Machine"""

prob_svc= svm.predict_proba(X_test)
auc_svc = metrics.roc_auc_score(y_test,prob_svc[:,1])
fprsvc,tprsvc,_= roc_curve(y_test,prob_svc[:,1])
roc_auc_svc=auc(fprsvc,tprsvc)
plt.plot(fprsvc,tprsvc,label = "AUC = %.2f" % auc_svc)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for SVM")
plt.plot([0,1],[0,1],label='baseline',
         linestyle='--')
plt.legend()
plt.show()

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
model3=lr.fit(X_train,y_train)
predLR=model3.predict(X_test)

cmLR=confusion_matrix(y_test,predLR)
cmLR

sns.heatmap(cmLR,annot=True,cmap='rainbow')

print(classification_report(y_test,predLR))

print("Accuracy Score : ",accuracy_score(y_test,predLR))

accuracyLR=accuracy_score(y_test,predLR)
accuracy.append(accuracyLR)

#lr_probs = model3.predict_proba(X_test)
print("f1-score : ",f1_score(y_test,predLR))

f1LR= f1_score(y_test,predLR)
F1_Score.append(f1LR)

print("r2-score : ",r2_score(y_test,predLR))

r2LR=r2_score(y_test,predLR)
R2_Score.append(r2LR)

print("Precision: ", metrics.precision_score(y_test, predLR))

precisionLR= metrics.precision_score(y_test, predLR)
PRECISION.append(precisionLR)

print("Recall: ", metrics.recall_score(y_test, predLR))

recallLR=metrics.recall_score(y_test, predLR)
RECALL.append(recallLR)

"""ROC_AUC Curve for Logistic Regression"""

prob_lr=lr.predict_proba(X_test)
auc_lr = metrics.roc_auc_score(y_test,prob_lr[:,1])
fprlr,tprlr,_ = roc_curve(y_test,prob_lr[:,1])
roc_auc=auc(fprlr,tprlr)
plt.plot(fprlr,tprlr,label = "AUC = %.2f" % auc_lr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Logistic Regression")
plt.plot([0,1],[0,1],label='baseline',
         linestyle='--')
plt.legend()
plt.show()

"""Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier
x=GradientBoostingClassifier(learning_rate=0.1,n_estimators=100)
modelx=x.fit(X_train,y_train)
predx=modelx.predict(X_test)
cmx=confusion_matrix(y_test,predx)
cmx

sns.heatmap(cmx,annot=True,cmap="YlGnBu")

print(classification_report(y_test,predx))

print("Accuracy Score : ",accuracy_score(y_test,predx))

accuracyGB=accuracy_score(y_test,predx)
accuracy.append(accuracyGB)

print("r2-score : ",r2_score(y_test,predx))

r2scoreGB=r2_score(y_test,predx)
R2_Score.append(r2scoreGB)

print("Precision: ", metrics.precision_score(y_test, predx))

precisionGB=metrics.precision_score(y_test, predx)
PRECISION.append(precisionGB)

print("Recall: ", metrics.recall_score(y_test, predx))

recallGB=metrics.recall_score(y_test, predx)
RECALL.append(recallGB)

print("F1_SCORE :",f1_score(y_test,predx))

"""ROC_AUC Curve for Gradient Boosting"""

prob_GBC= modelx.predict_proba(X_test)
auc_GBC = metrics.roc_auc_score(y_test,prob_GBC[:,1])
fprGBC,tprGBC,_= roc_curve(y_test,prob_GBC[:,1])
roc_auc_GBC=auc(fprGBC,tprGBC)
plt.plot(fprGBC,tprGBC,label = "AUC = %.2f" % auc_GBC)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Gradient Boosting")
plt.plot([0,1],[0,1],label='baseline',
         linestyle='--')
plt.legend()
plt.show()

f1GB=f1_score(y_test,predx)
F1_Score.append(f1GB)

accuracy

accuracy_list=[element * 100 for element in accuracy]
print(accuracy_list)

algorithms=['Randomforest','DecisionTree','KNN','Naive Bayes','Support Vector Machine','Logistic Regression','Gradient Boosting']
algorithms

sns.set_style("whitegrid")
plt.figure(figsize=(16,5))
plt.yticks(np.arange(0,100,10))
plt.ylabel("Accuracy in %")
plt.xlabel("Algorithm Name")
sns.barplot(x=algorithms,y=accuracy_list)
plt.show()

"""#R2_Score"""

r2_list=[element * 100 for element in R2_Score]
print(r2_list)

sns.set_style("whitegrid")
plt.figure(figsize=(16,5))
plt.yticks(np.arange(0,100,10))
plt.ylabel("R2_SCORE in %")
plt.xlabel("Algorithm Name")
sns.barplot(y=r2_list,x=algorithms)
plt.show()

"""#F1_Score"""

f1_list=[element * 100 for element in F1_Score]
print(f1_list)

sns.set_style("whitegrid")
plt.figure(figsize=(16,5))
plt.yticks(np.arange(0,100,10))
plt.ylabel("F1_SCORE in %")
plt.xlabel("Algorithm Name")
sns.barplot(y=f1_list,x=algorithms)
plt.show()

p_list=[element * 100 for element in PRECISION]
print(p_list)

sns.set_style("whitegrid")
plt.figure(figsize=(16,5))
plt.yticks(np.arange(0,100,10))
plt.ylabel("PRECISION in %")
plt.xlabel("Algorithm Name")
sns.barplot(y=p_list,x=algorithms)
plt.show()

multiplied_list = [element * 100 for element in RECALL]

print(multiplied_list)

"""Plotting the results"""

sns.set_style("whitegrid")
plt.figure(figsize=(16,5))
plt.yticks(np.arange(0,100,10))
plt.ylabel("RECALL in %")
plt.xlabel("Algorithm Name")
sns.barplot(y=multiplied_list,x=algorithms)
plt.show()